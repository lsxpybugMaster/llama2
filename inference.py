from typing import Optional
import torch
import time
from pathlib import Path
import json
from sentencepiece import SentencePieceProcessor
from tqdm import tqdm

# model.py
from model import ModelArgs, Transformer


#ğŸ¦™
class LLaMA:
    def __init__(
        self,
        model: Transformer,
        tokenizer: SentencePieceProcessor,
        model_args: ModelArgs
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.args = model_args

    # è¯»å–æƒé‡æ–‡ä»¶llama-2-7b
    @staticmethod
    def build(
        checkpoints_dir: str,
        tokenizer_path: str,
        load_model: bool,
        max_seq_len: int,
        max_batch_size: int,
        device: str
    ):
        # è®¡æ—¶
        prev_time = time.time()

        # åŠ è½½æƒé‡æ–‡ä»¶
        if load_model:
            checkpoints = sorted(Path(checkpoints_dir).glob("*.pth"))
            assert len(checkpoints) > 0, f"{checkpoints_dir} è·¯å¾„ä¸‹æœªå‘ç°æƒé‡æ–‡ä»¶"
            ckpt_path = checkpoints[0]
            print(f"åŠ è½½æƒé‡æ–‡ä»¶ {ckpt_path}")
            # å¾—åˆ°æƒé‡å­—å…¸
            checkpoint = torch.load(ckpt_path,map_location='cpu')

            print(f"åŠ è½½æƒé‡èŠ±è´¹æ—¶é—´: {time.time() - prev_time:.2f}s")
            prev_time = time.time()

        # åŠ è½½é…ç½®æ–‡ä»¶
        with open(Path(checkpoints_dir) / "params.json", "r") as f:
            params = json.loads(f.read())
            
        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            device=device,
            **params  #ä¼ å…¥å…¶ä»–é…ç½®è‡ªåŠ¨åŒ¹é…å¯¹åº”é¡¹
        )

        # åŠ è½½åˆ†è¯å™¨
        tokenizer = SentencePieceProcessor()
        tokenizer.load(tokenizer_path)
        model_args.vocab_size = tokenizer.vocab_size()

        # åŠç²¾åº¦è®¡ç®—
        if device == "cuda":
            torch.set_default_tensor_type(torch.cuda.HalfTensor)
        else:
            torch.set_default_tensor_type(torch.BFloat16Tensor)

        # âš ï¸Pytorché«˜ç‰ˆæœ¬ä¸Šé¢çš„ä»£ç ä¼šæŠ¥å‡ºè­¦å‘Š
        # if device == "cuda":
        #     torch.set_default_dtype(torch.float16)
        #     torch.set_default_device("cuda")
        # else:
        #     torch.set_default_dtype(torch.bfloat16)
        #     torch.set_default_device("cpu")

        print(f"å°†æ¨¡å‹éƒ¨ç½²è‡³è®¾å¤‡: {device}")
        model = Transformer(model_args).to(device)
        print(f"å®Œæˆå°†æ¨¡å‹éƒ¨ç½²è‡³è®¾å¤‡")

        # çœŸæ­£åœ°é€KeyåŒ¹é…åŠ è½½æƒé‡
        if load_model:
            # åˆ é™¤æ—‹è½¬ä½ç½®ç¼–ç æ•°æ®,æˆ‘ä»¬ç›´æ¥è®¡ç®—
            # æƒé‡æ˜¯å­—å…¸
            
            del checkpoint['rope.freqs']
            
            print(f"åŠ è½½æƒé‡å­—å…¸ä¸­")
            model.load_state_dict(checkpoint,strict=True)
            print(f"åŠ è½½äº†æƒé‡å­—å…¸,è€—æ—¶{time.time() - prev_time:.2f}s")

        return LLaMA(model,tokenizer,model_args)
    
    
    ## top_p é¢„æµ‹
    ## æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ»¡è¶³ â€œç´¯ç§¯æ¦‚ç‡ - å½“å‰æ¦‚ç‡ > Pâ€ çš„ä½ç½®
    def _sample_top_p(self, probs, p):
        # (B, vocab_size)
        # ä¸ºäº†æ–¹ä¾¿æ‰¾å‰xä¸ªæ€»å’Œæ¥è¿‘pçš„tokenè¿›è¡Œéšæœºï¼Œæˆ‘ä»¬å…ˆæ’åº
        # ç”±äºæ’åºåä½ç½®å˜åŒ–,æ‰€ä»¥ä½¿ç”¨probs_idxå­˜å‚¨æ’åºå‰ä½ç½®
        prob_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
        # å‰ç¼€å’Œ
        prob_sum = torch.cumsum(prob_sort, dim = -1)
        # prob_sum - prob_sort ç›¸å½“äºå®ç°å³ç§»
        '''
        prob_sort [5, 4,  3,  2,  1]
        prob_sum  [5, 9, 12, 14, 15]
        sum - sort[0, 5,  9, 12, 14]
        P = 10
        mask =    [F, F,  F,  T,  T]
        prob[mask][5, 4,  3,  0,  0]
        '''
        mask = prob_sum - prob_sort > p
        # å¯¹äºä¸ºTçš„,æˆ‘ä»¬ä¸å†éœ€è¦,å°†å…¶ç½®ä¸º0
        prob_sort[mask] = 0.0
        # ä¸€äº›å…ƒç´ è¢«æŠ¹é™¤ä¸º0å,éœ€è¦é‡æ–°å½’ä¸€åŒ–
        prob_sort.div_(prob_sort.sum(dim = -1, keepdim=True))
        # éšæœºé€‰å–,è¾“å…¥å‘é‡çš„å€¼ä»£è¡¨é€‰å–æ¦‚ç‡(å› æ­¤0æ°¸è¿œä¸é€‰)
        next_token = torch.multinomial(prob_sort,num_samples=1)
        # å°†ä½ç½®æ˜ å°„å›åŸä½ç½®(æ’åºå‰)
        next_token = torch.gather(probs_idx,-1,next_token)
        return next_token


    # Tokené¢„æµ‹
    def text_completion(
        self,
        prompts: list[str], # promptså¯ä»¥æˆbatch
        temperature: float = 0.6, # æ¸©åº¦è°ƒèŠ‚logitçš„ä¿¡å¿ƒåº¦
        top_p: float = 0.9, # top_pé¢„æµ‹æ–¹å¼
        max_gen_len: Optional[int] = None
    ):
        if max_gen_len is None:
            max_gen_len = self.args.max_seq_len - 1

        ## prompt -> tokens
        prompt_tokens = [
            self.tokenizer.encode(prompt,out_type=int,add_bos=True,add_eos=False) 
            for prompt in prompts  # é€batchè½¬æ¢
        ]

        ## å¤„ç†é•¿åº¦ä¿¡æ¯
        batch_size = len(prompt_tokens)
        assert batch_size <= self.args.max_batch_size, f"è¾“å…¥çš„promptå¥å­æ•°éœ€è¦ <= {self.args.max_batch_size}"

        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)
        assert max_prompt_len <= self.args.max_seq_len, f"promptçš„tokené•¿åº¦éœ€è¦ <= {self.args.max_seq_len}"

        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)

        ## åˆå§‹åŒ–tokenåºåˆ—
        # å…¨éƒ¨å¡«å……ä¸º[PAD]
        pad_id = self.tokenizer.pad_id()
        tokens = torch.full((batch_size,total_len),pad_id,dtype=torch.long,device=device)

        for k,t in enumerate(prompt_tokens):
            # å°†tokenåºåˆ—ä¸­æ·»åŠ åˆå§‹prompt token
            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long,device=device)

        # æ¯ä¸ªtokenåºåˆ—æ˜¯å¦åˆ°è¾¾eos(ç”Ÿæˆå®Œæ¯•)
        eos_reached = torch.tensor([False] * batch_size, device=device)
        # PAD Mask, Trueä»£è¡¨å½“å‰ä½ç½®ä¸ºæœ‰æ•ˆtoken
        prompt_tokens_mask = tokens != pad_id

        ## å‡†å¤‡é¢„æµ‹token
        cur_iterator = tqdm(range(1, total_len),desc="Generating tokens")
        for cur_pos in cur_iterator:
            ## å‰å‘è®¡ç®—
            with torch.no_grad():
                # ä½¿ç”¨KV-cache : ä»…éœ€ä¼ å…¥1ä¸ªtoken
                # RoPE : éœ€ä¼ å…¥å½“å‰tokenä½ç½®
                # logits : (B, seq_len = 1?, vocab_size)
                logits = self.model.forward(tokens[:, cur_pos - 1 : cur_pos], cur_pos)
            if temperature > 0:
                # ç»™logitsåŠ å…¥æ¸©åº¦
                probs = torch.softmax(logits[:, -1] / temperature, dim = -1)
                next_token = self._sample_top_p(probs,top_p)
            else:
                # ä¸åŠ æ¸©åº¦å°±åšè´ªå©ªæœç´¢
                next_token = torch.argmax(logits[:, -1],dim = -1)

            # (B, 1) -> (B,)
            next_token = next_token.reshape(-1)
            # æ˜¯[pad]æ‰åštokenæ›¿æ¢,å¦åˆ™ä¿ç•™åŸtoken
            # ç”Ÿæˆåº”ä»æ¯ä¸ª prompt çš„å®é™…ç»“æŸä½ç½®å¼€å§‹
            # whereé€»è¾‘ç±»ä¼¼ä¸‰å…ƒè¡¨è¾¾å¼
            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:,cur_pos], next_token)
            # ğŸ¤—ç»ˆäºç”Ÿæˆè¯¥ä½ç½®token
            tokens[:, cur_pos] = next_token
            # |= è¡¨ç¤ºç´¯ç§¯é€»è¾‘æˆ–,ç”¨äºåå¤åˆ¤æ–­æ˜¯å¦åˆ°è¾¾[eos],ä¸”åˆ°è¾¾åä¸å†å˜åŒ–
            # åç»­é€»è¾‘:  ä¸ä¸ºåˆå§‹prompt & å½“å‰é¢„æµ‹è¯ä¸º[eos]
            eos_reached |= (~prompt_tokens_mask[:,cur_pos]) & (next_token == self.tokenizer.eos_id)
            if all(eos_reached):
                break
        
        ## ğŸ¤— åˆ°è¾¾è¯¥ä½ç½®è¯´æ˜æ‰€æœ‰batchç”Ÿæˆå®Œæ¯•
        out_tokens = []
        out_text = []
        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):
            # æ„æ€æ˜¯æ‰¾åˆ°ç¬¬ä¸€ä¸ª[eos] å°±ç«‹åˆ»å¤„ç† (æœ‰å¯èƒ½ç”Ÿæˆå¤šä¸ªeos)
            if self.tokenizer.eos_id in current_prompt_tokens:
                # æ‰¾eoså¯¹åº”ç´¢å¼•
                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)
                # æˆªå–æœ‰æ•ˆéƒ¨åˆ† -> [eos]
                current_prompt_tokens = current_prompt_tokens[:eos_idx]
            # åŠ å…¥ä¸€å¥å®Œæ•´çš„tokenç­”æ¡ˆ
            out_tokens.append(current_prompt_tokens)
            out_text.append(self.tokenizer.decode(current_prompt_tokens))
        return (out_tokens, out_text)   
    


if __name__ == '__main__':
    
    torch.manual_seed(0)

    allow_cuda = False
    device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'

    # batch = 4 
    prompts = [
        "Simply put, the theory of relativity states that ",
        
        "If Google was an Italian company founded in Milan, it would",
        
        # Few shot promt
        """Translate English to French:
        
        sea otter => loutre de mer
        peppermint => menthe poivrÃ©e
        plush girafe => girafe peluche
        cheese =>""",
        
        # Zero shot prompt
        """Tell me if the following formula is correct:
        Formula: a - b = b - a
        Decision: incorrect
        Formula: a + b = b + a
        Decision: 
        """
    ]

    model = LLaMA.build(
        checkpoints_dir='llama-2-7b/',
        tokenizer_path=str(Path('llama-2-7b') / 'tokenizer.model'),
        load_model=True,
        max_seq_len=1024,
        max_batch_size=len(prompts),
        device=device
    )

    out_tokens, out_texts = (model.text_completion(prompts,max_gen_len=64))
    assert len(out_texts) == len(prompts) , "è¾“å‡ºbatchä¸è¾“å…¥batchä¸ä¸€è‡´"
    for i in range(len(out_texts)):
        print(f'{out_texts[i]}')
        print('-' * 50)